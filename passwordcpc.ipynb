{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/rockyouwith-similarity-model-info/lengthCounts.pickle\n/kaggle/input/rockyouwith-similarity-model-info/negative_passwords.dic\n/kaggle/input/rockyouwith-similarity-model-info/rockyou-test.txt\n/kaggle/input/rockyouwith-similarity-model-info/stringCounts.pickle\n/kaggle/input/rockyouwith-similarity-model-info/rockyou-train.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from __future__ import print_function\nfrom tqdm import tqdm\nimport sys\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as path_effects\nimport mmap\nimport pickle as pic\nfrom time import *\nfrom functools import reduce\nimport random\nimport time\nimport logging\nfrom timeit import default_timer as timer\n%matplotlib inline","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"directory = '/kaggle/input/rockyouwith-similarity-model-info/'","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Similarity Model-Trie**","metadata":{}},{"cell_type":"code","source":"class TrieNode:\n    # Trie node class\n    def __init__(self):\n        self.children = {}\n        self.count = 0\n\nclass Trie:      \n    # Trie data structure class\n    def __init__(self):\n        self.root = TrieNode()\n        self.stringCounts = {}\n        self.lengthCounts = {}\n  \n    def insert(self,key):\n        cur = self.root\n        length = len(key)\n        for idx,c in enumerate(key):\n            if idx+1 not in self.lengthCounts:\n                self.lengthCounts[idx+1] = 0\n            if c not in cur.children:\n                cur.children[c] = TrieNode()\n            self.lengthCounts[idx+1] += 1\n            cur.count += 1\n            cur = cur.children[c]\n        cur.count += 1\n    \n    def recurseStringCounts(self,cur,string):\n        if string!='':\n            self.stringCounts[string] = cur.count\n        if cur.children:\n            for key in cur.children:\n                self.recurseStringCounts(cur.children[key], string+chr(key))\n\n    def constructStringCounts(self):\n        cur = self.root\n        string = ''\n        self.stringCounts = {}\n        self.recurseStringCounts(cur,string)\n        \n    def similarity(self, key, keyLength ):  \n        # Gets similarity score of a string\n        score = 0\n        string = ''\n        for idx, c in enumerate(key):\n            string += key[idx]\n            if string not in self.stringCounts:\n                break\n            length = len(string)\n            lengthRatio = length/keyLength\n            stringLengthCountRatio = self.stringCounts[string]/self.lengthCounts[length]\n            score = max(score, stringLengthCountRatio*lengthRatio+self.similarity(key[idx+1:],keyLength))\n        return score\nsimilarityModel = Trie()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_num_lines(file_path):\n    fp = open(file_path, \"r+\")\n    buf = mmap.mmap(fp.fileno(), 0)\n    lines = 0\n    while buf.readline():\n        lines += 1\n    return lines\n    fp.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists(directory+\"lengthCounts.pickle\"):\n    similarityModel.lengthCounts = pic.load(open(directory+\"lengthCounts.pickle\", \"rb\" ))\nelse:\n    file = open(directory+'rockyou-train.txt','rb')\n    for line in tqdm(file, total=get_num_lines(directory+'rockyou-train.txt')):\n        password = line\n        for start in range(len(password)):\n            for end in range(start+1,len(password)):\n                similarityModel.insert(password[start:end])\n    file.close()\n    file = open(directory+\"lengthCounts.pickle\", \"wb\" )\n    pic.dump(similarityModel.lengthCounts,file)\n    file.close()\n    \nif os.path.exists(directory+\"stringCounts.pickle\"):\n    similarityModel.stringCounts = pic.load(open(directory+\"stringCounts.pickle\", \"rb\" ))\nelse:\n    similarityModel.constructStringCounts()\n    file = open(directory+\"stringCounts.pickle\", \"wb\" )\n    pic.dump(similarityModel.stringCounts,file)\n    file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Negative Samples Generator**","metadata":{}},{"cell_type":"code","source":"def getRandomSample(passSizeRange):\n    passSize = random.randint(passSizeRange[0],passSizeRange[1])\n    password = ''\n    symbolicCharacter = list(range(32,ord('0')))+list(range(ord('9')+1,ord('A')))+list(range(ord('Z')+1,ord('a')))+list(range(ord('z')+1,128))\n    symCharCt = len(symbolicCharacter)\n    for __ in range(passSize):\n        charType = random.randint(1,4)\n        if charType==1: # lower case alphabet\n            password+=chr(random.randint(ord('a'),ord('z')))\n        elif charType==2:\n            password+=chr(random.randint(ord('A'),ord('Z')))\n        elif charType==3:\n            password+=chr(random.randint(ord('0'),ord('9')))\n        else:\n            \n            password+=chr(random.choice(symbolicCharacter))\n    return password","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def negativeSamples(threshold, samplesize,passSizeRange, chr_to_idx):\n    passwords = set()\n    for __ in range(samplesize):\n        testPass = getRandomSample(passSizeRange)\n        while testPass not in passwords and similarityModel.similarity(testPass, len(testPass))>=threshold:\n            testPass = getRandomSample(passSizeRange)\n        passwords.add(testPass)\n    return torch.LongTensor([[chr_to_idx[c] for c in password]+[chr_to_idx[' ']]*(passSizeRange[1]-len(password)) for password in passwords])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Getting Pre-Generated Passwords**","metadata":{}},{"cell_type":"code","source":"passwords = list(pic.load(open( directory+\"negative_passwords.dic\", \"rb\" )))\nprint('Password length:',len(passwords))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CPC Model**","metadata":{}},{"cell_type":"code","source":"class CDCK2(nn.Module):\n    def __init__(self, passwords, vocab_size, embedding_dim, seq_len, threshold, negativeCounts, chr_to_idx, device):\n\n        super(CDCK2, self).__init__()\n        # Pre generated Negative Passwords\n        self.passwords = passwords\n        \n        # Device\n        self.device = device\n        \n        # Embedding parameters\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.chr_to_idx = chr_to_idx\n        \n        # negative sample parameters\n        self.seq_len = seq_len # =16 \n        self.threshold = threshold        \n        self.negativeCounts = negativeCounts\n        \n        # timestep calculation\n        self.timestep = self.seq_len*self.embedding_dim//8 # 16*8/8 = 16, here 8 is a downsampling factor = 8\n        self.t_samples = ((self.negativeCounts+1)*self.seq_len*self.embedding_dim)//8 - self.timestep-1\n        \n        # Hidden State\n        self.hidden_state = torch.zeros(1,1, 256).to(device)\n        \n        # Layers\n        self.embedding = nn.Embedding(self.vocab_size,self.embedding_dim)\n        self.encoder = nn.Sequential( # downsampling factor = 8\n            nn.Conv1d(1, 512, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 512, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 512, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True)\n        )\n        self.gru = nn.GRU(512, 256, num_layers=1, bidirectional=False, batch_first=True)\n        self.Wk  = nn.ModuleList([nn.Linear(256, 512) for i in range(self.timestep)])\n        self.softmax  = nn.Softmax()\n        self.lsoftmax = nn.LogSoftmax()\n\n        def _weights_init(m):\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # initialize gru\n        for layer_p in self.gru._all_weights:\n            for p in layer_p:\n                if 'weight' in p:\n                    nn.init.kaiming_normal_(self.gru.__getattr__(p), mode='fan_out', nonlinearity='relu')\n\n        self.apply(_weights_init)\n\n    def init_hidden(self, batch_size, use_gpu=True):\n        if use_gpu: return torch.zeros(1,1, 256).cuda()\n        else: return torch.zeros(1, 1, 256)\n    \n    def forward(self, x):        \n        batch = x.size()[0]\n        # input 512 x 1 x 16\n        emb = self.embedding(x) # 512 x 1 x 16 x 8\n        emb = emb.reshape(batch,1, 1*self.seq_len*self.embedding_dim) # 512 x 1 x 128\n        z = self.encoder(emb) # 512 x 512 x 16\n        forward_seq = z.transpose(1,2) # 512 x 16 x 512\n        forward_seq = forward_seq.reshape(1,batch*self.seq_len,512) # 1 x(512*16) x 512\n        \n        output, hidden = self.gru(forward_seq, self.hidden_state) # 512 x 16 x 256, 1 x(512*16) x 256\n        \n        output = output.reshape(batch, self.seq_len, 256) # 512 x 16 x 256\n        c_t = output[:,-1,:].view(batch, 256) # 512 x 256\n        pred = torch.empty((self.timestep,batch,512)).float().to(self.device) # e.g. size 16 x 512 x 512\n        for i in np.arange(0, self.timestep):\n            linear = self.Wk[i]\n            pred[i] = linear(c_t) # Wk*c_t e.g. size 512 x 512\n        nce = 0 # average over timestep and batch\n        for i in np.arange(0, self.timestep):\n            total = torch.mm(z[:,:,i], torch.transpose(pred[i],0,1)) # e.g. (512 x 512) x ( 512 x 512) = 512 x 512,  \n            correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0), torch.arange(0, batch).to(self.device))) # correct is a tensor\n            nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n        \n        nce /= -1.*batch*self.timestep\n        accuracy = 1.*correct.item()/batch\n        return accuracy, nce, hidden\n        \n        \n\n    def predict(self, x, hidden):\n        batch = x.size()[0]\n        # input sequence is N*C*L, e.g. 64 x 1 x 16\n        \n        emb = self.embedding(x) # 64 x 1 x 16 x 8\n        emb = emb.reshape(batch,1, 1*self.seq_len*self.embedding_dim) # 64 x 1 x 128\n        # sequence is N*C*L, e.g. 32*1*128\n        \n        z = self.encoder(x) # 64 x 512 x 16\n        \n        return z.reshape(batch, 512*self.timestep)","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Training and validation functions**","metadata":{}},{"cell_type":"code","source":"def trainCPC(model, device, train_loader, optimizer, epoch, batch_size):\n    model.train()\n    data_len = len(train_loader.dataset)\n    n_batches = data_len//batch_size\n    log_interval = n_batches//20\n    for batch_idx, data in enumerate(train_loader):\n        if len(data['xVect'])==batch_size:\n            data = data['xVect'].long().unsqueeze(1).to(device) # add channel dimension\n            optimizer.zero_grad()\n            acc, loss, hidden = model(data)\n            loss.backward()\n            optimizer.step()\n#             lr = optimizer.update_learning_rate()\n            if batch_idx % log_interval == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tAccuracy: {:.4f}\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                    100. * batch_idx / len(train_loader),  acc, loss.item()))\n            model.hidden_state = hidden.detach()","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def validationCPC(model, device, data_loader, batch_size):\n    model.eval()\n    total_loss = 0\n    total_acc  = 0 \n\n    with torch.no_grad():\n        for batch_idx, data in enumerate(data_loader):\n            if len(data['xVect'])==batch_size:\n                data = data['xVect'].long().unsqueeze(1).to(device) # add channel dimension\n                #hidden = model.init_hidden(len(data), use_gpu=True)\n                acc, loss, __ = model(data)\n                total_loss += len(data) * loss \n                total_acc  += len(data) * acc\n\n    total_loss /= len(data_loader.dataset) # average loss\n    total_acc  /= len(data_loader.dataset) # average acc\n\n    print('===> Validation set: Average loss: {:.4f}\\tAccuracy: {:.4f}\\n'.format(\n                total_loss, total_acc))\n\n    return total_acc, total_loss","metadata":{"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# **Logging function**","metadata":{}},{"cell_type":"markdown","source":"# Model training scheduling Optimizer","metadata":{}},{"cell_type":"code","source":"def setup_logs(save_dir, run_name):\n    # initialize logger\n    logger = logging.getLogger(\"cdc\")\n    logger.setLevel(logging.INFO)\n    # create the logging file handler\n    log_file = os.path.join(save_dir, run_name + \".log\")\n    fh = logging.FileHandler(log_file)\n    # create the logging console handler\n    ch = logging.StreamHandler()\n    # format\n    formatter = logging.Formatter(\"%(asctime)s - %(message)s\")\n    fh.setFormatter(formatter)\n    # add handlers to logger object\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n    return logger","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ScheduledOptim(object):\n    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n\n    def __init__(self, optimizer, n_warmup_steps):\n        self.optimizer = optimizer\n        self.d_model = 128 \n        self.n_warmup_steps = n_warmup_steps\n        self.n_current_steps = 0 \n        self.delta = 1\n\n    def state_dict(self):\n        self.optimizer.state_dict()\n\n    def step(self):\n        \"\"\"Step by the inner optimizer\"\"\"\n        self.optimizer.step()\n\n    def zero_grad(self):\n        \"\"\"Zero out the gradients by the inner optimizer\"\"\"\n        self.optimizer.zero_grad()\n\n    def increase_delta(self):\n        self.delta *= 2\n\n    def update_learning_rate(self):\n        \"\"\"Learning rate scheduling per step\"\"\"\n\n        self.n_current_steps += self.delta\n        new_lr = np.power(self.d_model, -0.5) * np.min([\n            np.power(self.n_current_steps, -0.5),\n            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = new_lr\n        return new_lr","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Snapshot**","metadata":{}},{"cell_type":"code","source":"def snapshot(dir_path, run_name, state):\n    snapshot_file = os.path.join(dir_path,\n                    run_name + '-model_best.pth')\n    \n    torch.save(state, snapshot_file)\n#     logger.info(\"Snapshot saved to {}\\n\".format(snapshot_file))","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset**","metadata":{}},{"cell_type":"code","source":"class PasswordDataset(Dataset):\n    \"\"\"Password dataset.\"\"\"\n\n    def __init__(self,  source='./Dataset/', dataset_name='rockyou', dataset_type='train', maxlength=16, transform=None):\n        \"\"\"\n        Args:\n            source (string): Path to the benign dataset.\n            epsilons (list): List of all epsilon values for FGSM attack\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n            init_model (DLS_Model) : The model \n        \"\"\"\n\n        # character to index map, considering all printable ASCII - (32-127) \n        \n        self.chr_to_idx = {chr(x):(x-32) for x in range(32,128)}\n        self.idx_to_chr = {(x-32):chr(x) for x in range(32,128)}\n        self.path = source+dataset_name+'-'+dataset_type+\".txt\"\n        with open(self.path, 'r',encoding='latin1') as f:\n            self.passwords = tuple(filter(lambda y: reduce(lambda a,b:a&b, [32<=ord(c)<=127 for c in y]) ,map(lambda x:x.strip()[:maxlength],f.readlines())))\n\n    def __len__(self):\n        return len(self.passwords)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        return {'passString':self.passwords[idx],'xVect':torch.LongTensor([self.chr_to_idx[c] for c in (self.passwords[idx]+' '*(maxlength-len(self.passwords[idx])))])}","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# **CPC Variables**","metadata":{}},{"cell_type":"code","source":"batch_size = 256\nlogging_dir = '/kaggle/working' #+'logging/CPC/'\nvocab_size = 127-32+1\nembedding_dim = 8\nseq_len = 16\nthreshold = 0.1\nnegativeCounts = 7\nepochs = 25\nmaxlength=16\nrun_name = \"cdc\" + time.strftime(\"-%Y-%m-%d_%H_%M_%S\")\nprint(run_name)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"cdc-2021-05-13_06_13_27\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **getting train and test Dataset**","metadata":{}},{"cell_type":"code","source":"trainDataset = PasswordDataset(source=directory, dataset_type='train', maxlength=maxlength)\ntrainDataset[0]","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'passString': 'antonio25',\n 'xVect': tensor([65, 78, 84, 79, 78, 73, 79, 18, 21,  0,  0,  0,  0,  0,  0,  0])}"},"metadata":{}}]},{"cell_type":"code","source":"testDataset = PasswordDataset(source=direcotry, dataset_type='test',maxlength=maxlength)\ntestDataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Split by train-validation**","metadata":{}},{"cell_type":"code","source":"train_val_size = 0.15\ntrain_val_idx, remaining_idx = train_test_split(list(range(len(trainDataset))), test_size=1-train_val_size)\ntrain_val_set = Subset(trainDataset, train_val_idx)\nval_split = 0.33 \ntrain_idx, val_idx = train_test_split(list(range(len(train_val_set))), test_size=val_split)\ntraining_set = Subset(train_val_set, train_idx)\nvalidation_set = Subset(train_val_set, val_idx)\nlen(training_set),len(validation_set),training_set[0],validation_set[0]","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(2141698,\n 1054867,\n {'passString': 'mikehe',\n  'xVect': tensor([77, 73, 75, 69, 72, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])},\n {'passString': 'ADRIANNA',\n  'xVect': tensor([33, 36, 50, 41, 33, 46, 46, 33,  0,  0,  0,  0,  0,  0,  0,  0])})"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Setting GPU/CPU informtion**","metadata":{}},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nprint('use_cuda is', use_cuda)\nglobal_timer = timer() # global timer\n#logger = setup_logs(logging_dir, run_name) # setup logs\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n## Loading the dataset\nparams = {'num_workers': 0, 'pin_memory': False} if use_cuda else {}","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"use_cuda is True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Getting Loaders for training and validation**","metadata":{}},{"cell_type":"code","source":"print('===> loading train, validation and eval dataset')\ntrain_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, **params) # set shuffle to True\nvalidation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, **params) # set shuffle to False","metadata":{"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"===> loading train, validation and eval dataset\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Training and Validation process**","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CDCK2([], vocab_size, embedding_dim, seq_len, threshold, negativeCounts, trainDataset.chr_to_idx, device).to(device)\noptimizer = optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()), lr = 1e-5, \n            betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True)\nmodel_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('### Model summary below###\\n {}\\n'.format(str(model)))\nprint('===> Model total parameter: {}\\n'.format(model_params))\n## Start training\nbest_acc = 0\nbest_loss = np.inf\nbest_epoch = -1 ","metadata":{"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"### Model summary below###\n CDCK2(\n  (embedding): Embedding(96, 8)\n  (encoder): Sequential(\n    (0): Conv1d(1, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n  )\n  (gru): GRU(512, 256, batch_first=True)\n  (Wk): ModuleList(\n    (0): Linear(in_features=256, out_features=512, bias=True)\n    (1): Linear(in_features=256, out_features=512, bias=True)\n    (2): Linear(in_features=256, out_features=512, bias=True)\n    (3): Linear(in_features=256, out_features=512, bias=True)\n    (4): Linear(in_features=256, out_features=512, bias=True)\n    (5): Linear(in_features=256, out_features=512, bias=True)\n    (6): Linear(in_features=256, out_features=512, bias=True)\n    (7): Linear(in_features=256, out_features=512, bias=True)\n    (8): Linear(in_features=256, out_features=512, bias=True)\n    (9): Linear(in_features=256, out_features=512, bias=True)\n    (10): Linear(in_features=256, out_features=512, bias=True)\n    (11): Linear(in_features=256, out_features=512, bias=True)\n    (12): Linear(in_features=256, out_features=512, bias=True)\n    (13): Linear(in_features=256, out_features=512, bias=True)\n    (14): Linear(in_features=256, out_features=512, bias=True)\n    (15): Linear(in_features=256, out_features=512, bias=True)\n  )\n  (softmax): Softmax(dim=None)\n  (lsoftmax): LogSoftmax(dim=None)\n)\n\n===> Model total parameter: 4274944\n\n","output_type":"stream"}]},{"cell_type":"code","source":"%%bash\npip install --no-index --find-links ../input/torchsummary torchsummary","metadata":{"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Looking in links: ../input/torchsummary\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Url '../input/torchsummary' is ignored. It is either a non-existing path or lacks a specific scheme.\nERROR: Could not find a version that satisfies the requirement torchsummary\nERROR: No matching distribution found for torchsummary\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-154174ca45d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pip install --no-index --find-links ../input/torchsummary torchsummary\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2397\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2399\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'pip install --no-index --find-links ../input/torchsummary torchsummary\\n'' returned non-zero exit status 1."],"ename":"CalledProcessError","evalue":"Command 'b'pip install --no-index --find-links ../input/torchsummary torchsummary\\n'' returned non-zero exit status 1.","output_type":"error"}]},{"cell_type":"code","source":"from torchsummary import summary\nprint(summary(model, (1, 1, 16), device=device))","metadata":{"trusted":true},"execution_count":39,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-ffdf4faf8429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torchsummary'","output_type":"error"}]},{"cell_type":"code","source":"for epoch in tqdm(range(1, epochs + 1)):\n    epoch_timer = timer()\n\n    # Train and validate\n    #trainXXreverse(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n    #val_acc, val_loss = validationXXreverse(args, model, device, validation_loader, args.batch_size)\n    trainCPC(model, device, train_loader, optimizer, epoch, batch_size)\n    val_acc, val_loss = validationCPC(model, device, validation_loader, batch_size)\n\n    # Save\n    if val_acc > best_acc: \n        best_acc = max(val_acc, best_acc)\n        snapshot(logging_dir, run_name, {\n            'epoch': epoch + 1,\n            'validation_acc': val_acc, \n            'state_dict': model.state_dict(),\n            'validation_loss': val_loss,\n            'optimizer': optimizer.state_dict(),\n        })\n        best_epoch = epoch + 1\n    elif epoch - best_epoch > 2:\n        optimizer.increase_delta()\n        best_epoch = epoch + 1\n\n    end_epoch_timer = timer()\n    print(\"#### End epoch {}/{}, elapsed time: {}\".format(epoch, epochs, end_epoch_timer - epoch_timer))\n\n## end \nend_global_timer = timer()\nprint(\"################## Success #########################\")\nprint(\"Total elapsed time: %s\" % (end_global_timer - global_timer))","metadata":{"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"  0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"},{"name":"stdout","text":"Train Epoch: 1 [0/2141698 (0%)]\tAccuracy: 0.0039\tLoss: 5.582710\nTrain Epoch: 1 [107008/2141698 (5%)]\tAccuracy: 0.0039\tLoss: 4.828536\nTrain Epoch: 1 [214016/2141698 (10%)]\tAccuracy: 0.0039\tLoss: 4.465310\nTrain Epoch: 1 [321024/2141698 (15%)]\tAccuracy: 0.0039\tLoss: 4.307797\nTrain Epoch: 1 [428032/2141698 (20%)]\tAccuracy: 0.0039\tLoss: 4.147477\nTrain Epoch: 1 [535040/2141698 (25%)]\tAccuracy: 0.0039\tLoss: 4.022017\nTrain Epoch: 1 [642048/2141698 (30%)]\tAccuracy: 0.0039\tLoss: 3.965002\nTrain Epoch: 1 [749056/2141698 (35%)]\tAccuracy: 0.0039\tLoss: 3.864173\nTrain Epoch: 1 [856064/2141698 (40%)]\tAccuracy: 0.0039\tLoss: 3.811401\nTrain Epoch: 1 [963072/2141698 (45%)]\tAccuracy: 0.0039\tLoss: 3.779273\nTrain Epoch: 1 [1070080/2141698 (50%)]\tAccuracy: 0.0039\tLoss: 3.669440\nTrain Epoch: 1 [1177088/2141698 (55%)]\tAccuracy: 0.0039\tLoss: 3.667117\nTrain Epoch: 1 [1284096/2141698 (60%)]\tAccuracy: 0.0039\tLoss: 3.661909\nTrain Epoch: 1 [1391104/2141698 (65%)]\tAccuracy: 0.0039\tLoss: 3.602517\nTrain Epoch: 1 [1498112/2141698 (70%)]\tAccuracy: 0.0039\tLoss: 3.574365\nTrain Epoch: 1 [1605120/2141698 (75%)]\tAccuracy: 0.0039\tLoss: 3.593320\nTrain Epoch: 1 [1712128/2141698 (80%)]\tAccuracy: 0.0039\tLoss: 3.509369\nTrain Epoch: 1 [1819136/2141698 (85%)]\tAccuracy: 0.0039\tLoss: 3.471271\nTrain Epoch: 1 [1926144/2141698 (90%)]\tAccuracy: 0.0039\tLoss: 3.467052\nTrain Epoch: 1 [2033152/2141698 (95%)]\tAccuracy: 0.0039\tLoss: 3.478600\nTrain Epoch: 1 [2140160/2141698 (100%)]\tAccuracy: 0.0039\tLoss: 3.436675\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 1/25 [28:20<11:20:17, 1700.72s/it]","output_type":"stream"},{"name":"stdout","text":"===> Validation set: Average loss: 3.4228\tAccuracy: 0.0039\n\n#### End epoch 1/25, elapsed time: 1700.714957487\nTrain Epoch: 2 [0/2141698 (0%)]\tAccuracy: 0.0039\tLoss: 3.410661\nTrain Epoch: 2 [107008/2141698 (5%)]\tAccuracy: 0.0039\tLoss: 3.429168\nTrain Epoch: 2 [214016/2141698 (10%)]\tAccuracy: 0.0039\tLoss: 3.381984\nTrain Epoch: 2 [321024/2141698 (15%)]\tAccuracy: 0.0039\tLoss: 3.391521\nTrain Epoch: 2 [428032/2141698 (20%)]\tAccuracy: 0.0039\tLoss: 3.383900\nTrain Epoch: 2 [535040/2141698 (25%)]\tAccuracy: 0.0039\tLoss: 3.349157\nTrain Epoch: 2 [642048/2141698 (30%)]\tAccuracy: 0.0039\tLoss: 3.346217\nTrain Epoch: 2 [749056/2141698 (35%)]\tAccuracy: 0.0039\tLoss: 3.302078\nTrain Epoch: 2 [856064/2141698 (40%)]\tAccuracy: 0.0039\tLoss: 3.326784\nTrain Epoch: 2 [963072/2141698 (45%)]\tAccuracy: 0.0039\tLoss: 3.270581\nTrain Epoch: 2 [1070080/2141698 (50%)]\tAccuracy: 0.0039\tLoss: 3.287774\nTrain Epoch: 2 [1177088/2141698 (55%)]\tAccuracy: 0.0039\tLoss: 3.332269\nTrain Epoch: 2 [1284096/2141698 (60%)]\tAccuracy: 0.0039\tLoss: 3.284407\nTrain Epoch: 2 [1391104/2141698 (65%)]\tAccuracy: 0.0039\tLoss: 3.239116\nTrain Epoch: 2 [1498112/2141698 (70%)]\tAccuracy: 0.0039\tLoss: 3.236035\nTrain Epoch: 2 [1605120/2141698 (75%)]\tAccuracy: 0.0039\tLoss: 3.265983\nTrain Epoch: 2 [1712128/2141698 (80%)]\tAccuracy: 0.0039\tLoss: 3.278358\nTrain Epoch: 2 [1819136/2141698 (85%)]\tAccuracy: 0.0039\tLoss: 3.274914\nTrain Epoch: 2 [1926144/2141698 (90%)]\tAccuracy: 0.0039\tLoss: 3.231068\nTrain Epoch: 2 [2033152/2141698 (95%)]\tAccuracy: 0.0039\tLoss: 3.199913\nTrain Epoch: 2 [2140160/2141698 (100%)]\tAccuracy: 0.0039\tLoss: 3.187158\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 2/25 [56:49<10:53:44, 1705.42s/it]","output_type":"stream"},{"name":"stdout","text":"===> Validation set: Average loss: 3.2233\tAccuracy: 0.0039\n\n#### End epoch 2/25, elapsed time: 1708.7146701870006\nTrain Epoch: 3 [0/2141698 (0%)]\tAccuracy: 0.0039\tLoss: 3.238401\nTrain Epoch: 3 [107008/2141698 (5%)]\tAccuracy: 0.0039\tLoss: 3.242163\nTrain Epoch: 3 [214016/2141698 (10%)]\tAccuracy: 0.0039\tLoss: 3.235314\nTrain Epoch: 3 [321024/2141698 (15%)]\tAccuracy: 0.0039\tLoss: 3.214135\nTrain Epoch: 3 [428032/2141698 (20%)]\tAccuracy: 0.0039\tLoss: 3.185091\nTrain Epoch: 3 [535040/2141698 (25%)]\tAccuracy: 0.0039\tLoss: 3.215107\nTrain Epoch: 3 [642048/2141698 (30%)]\tAccuracy: 0.0039\tLoss: 3.227096\nTrain Epoch: 3 [749056/2141698 (35%)]\tAccuracy: 0.0039\tLoss: 3.165308\nTrain Epoch: 3 [856064/2141698 (40%)]\tAccuracy: 0.0039\tLoss: 3.227514\nTrain Epoch: 3 [963072/2141698 (45%)]\tAccuracy: 0.0039\tLoss: 3.201295\nTrain Epoch: 3 [1070080/2141698 (50%)]\tAccuracy: 0.0039\tLoss: 3.172800\nTrain Epoch: 3 [1177088/2141698 (55%)]\tAccuracy: 0.0039\tLoss: 3.160968\nTrain Epoch: 3 [1284096/2141698 (60%)]\tAccuracy: 0.0039\tLoss: 3.199927\nTrain Epoch: 3 [1391104/2141698 (65%)]\tAccuracy: 0.0039\tLoss: 3.181705\nTrain Epoch: 3 [1498112/2141698 (70%)]\tAccuracy: 0.0039\tLoss: 3.175124\nTrain Epoch: 3 [1605120/2141698 (75%)]\tAccuracy: 0.0039\tLoss: 3.180945\nTrain Epoch: 3 [1712128/2141698 (80%)]\tAccuracy: 0.0039\tLoss: 3.157277\nTrain Epoch: 3 [1819136/2141698 (85%)]\tAccuracy: 0.0039\tLoss: 3.213354\nTrain Epoch: 3 [1926144/2141698 (90%)]\tAccuracy: 0.0039\tLoss: 3.144941\nTrain Epoch: 3 [2033152/2141698 (95%)]\tAccuracy: 0.0039\tLoss: 3.197144\nTrain Epoch: 3 [2140160/2141698 (100%)]\tAccuracy: 0.0039\tLoss: 3.198880\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 3/25 [1:24:27<10:17:19, 1683.61s/it]","output_type":"stream"},{"name":"stdout","text":"===> Validation set: Average loss: 3.1672\tAccuracy: 0.0039\n\n#### End epoch 3/25, elapsed time: 1657.6569914549991\nTrain Epoch: 4 [0/2141698 (0%)]\tAccuracy: 0.0039\tLoss: 3.163399\nTrain Epoch: 4 [107008/2141698 (5%)]\tAccuracy: 0.0039\tLoss: 3.173582\nTrain Epoch: 4 [214016/2141698 (10%)]\tAccuracy: 0.0039\tLoss: 3.173925\nTrain Epoch: 4 [321024/2141698 (15%)]\tAccuracy: 0.0039\tLoss: 3.104446\nTrain Epoch: 4 [428032/2141698 (20%)]\tAccuracy: 0.0039\tLoss: 3.162891\nTrain Epoch: 4 [535040/2141698 (25%)]\tAccuracy: 0.0039\tLoss: 3.108192\nTrain Epoch: 4 [642048/2141698 (30%)]\tAccuracy: 0.0039\tLoss: 3.192029\nTrain Epoch: 4 [749056/2141698 (35%)]\tAccuracy: 0.0039\tLoss: 3.151375\nTrain Epoch: 4 [856064/2141698 (40%)]\tAccuracy: 0.0039\tLoss: 3.153577\nTrain Epoch: 4 [963072/2141698 (45%)]\tAccuracy: 0.0039\tLoss: 3.103479\nTrain Epoch: 4 [1070080/2141698 (50%)]\tAccuracy: 0.0039\tLoss: 3.224144\nTrain Epoch: 4 [1177088/2141698 (55%)]\tAccuracy: 0.0039\tLoss: 3.135914\nTrain Epoch: 4 [1284096/2141698 (60%)]\tAccuracy: 0.0039\tLoss: 3.131752\nTrain Epoch: 4 [1391104/2141698 (65%)]\tAccuracy: 0.0039\tLoss: 3.145210\nTrain Epoch: 4 [1498112/2141698 (70%)]\tAccuracy: 0.0039\tLoss: 3.139606\nTrain Epoch: 4 [1605120/2141698 (75%)]\tAccuracy: 0.0039\tLoss: 3.135775\nTrain Epoch: 4 [1712128/2141698 (80%)]\tAccuracy: 0.0039\tLoss: 3.175662\nTrain Epoch: 4 [1819136/2141698 (85%)]\tAccuracy: 0.0039\tLoss: 3.115537\nTrain Epoch: 4 [1926144/2141698 (90%)]\tAccuracy: 0.0039\tLoss: 3.159796\nTrain Epoch: 4 [2033152/2141698 (95%)]\tAccuracy: 0.0039\tLoss: 3.156785\nTrain Epoch: 4 [2140160/2141698 (100%)]\tAccuracy: 0.0039\tLoss: 3.095904\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 4/25 [1:51:35<9:41:39, 1661.87s/it] ","output_type":"stream"},{"name":"stdout","text":"===> Validation set: Average loss: 3.1430\tAccuracy: 0.0039\n\n#### End epoch 4/25, elapsed time: 1628.530848773\nTrain Epoch: 5 [0/2141698 (0%)]\tAccuracy: 0.0039\tLoss: 3.169071\nTrain Epoch: 5 [107008/2141698 (5%)]\tAccuracy: 0.0039\tLoss: 3.165392\nTrain Epoch: 5 [214016/2141698 (10%)]\tAccuracy: 0.0039\tLoss: 3.127838\nTrain Epoch: 5 [321024/2141698 (15%)]\tAccuracy: 0.0039\tLoss: 3.138949\nTrain Epoch: 5 [428032/2141698 (20%)]\tAccuracy: 0.0039\tLoss: 3.159626\nTrain Epoch: 5 [535040/2141698 (25%)]\tAccuracy: 0.0039\tLoss: 3.120349\nTrain Epoch: 5 [642048/2141698 (30%)]\tAccuracy: 0.0039\tLoss: 3.154489\nTrain Epoch: 5 [749056/2141698 (35%)]\tAccuracy: 0.0039\tLoss: 3.168770\nTrain Epoch: 5 [856064/2141698 (40%)]\tAccuracy: 0.0039\tLoss: 3.165169\nTrain Epoch: 5 [963072/2141698 (45%)]\tAccuracy: 0.0039\tLoss: 3.155589\nTrain Epoch: 5 [1070080/2141698 (50%)]\tAccuracy: 0.0039\tLoss: 3.109652\nTrain Epoch: 5 [1177088/2141698 (55%)]\tAccuracy: 0.0039\tLoss: 3.111726\nTrain Epoch: 5 [1284096/2141698 (60%)]\tAccuracy: 0.0039\tLoss: 3.163754\nTrain Epoch: 5 [1391104/2141698 (65%)]\tAccuracy: 0.0039\tLoss: 3.157747\nTrain Epoch: 5 [1498112/2141698 (70%)]\tAccuracy: 0.0039\tLoss: 3.144343\nTrain Epoch: 5 [1605120/2141698 (75%)]\tAccuracy: 0.0039\tLoss: 3.135459\nTrain Epoch: 5 [1712128/2141698 (80%)]\tAccuracy: 0.0039\tLoss: 3.074382\nTrain Epoch: 5 [1819136/2141698 (85%)]\tAccuracy: 0.0039\tLoss: 3.129981\nTrain Epoch: 5 [1926144/2141698 (90%)]\tAccuracy: 0.0039\tLoss: 3.121771\nTrain Epoch: 5 [2033152/2141698 (95%)]\tAccuracy: 0.0039\tLoss: 3.173227\nTrain Epoch: 5 [2140160/2141698 (100%)]\tAccuracy: 0.0039\tLoss: 3.149698\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 4/25 [2:18:43<12:08:17, 2080.82s/it]","output_type":"stream"},{"name":"stdout","text":"===> Validation set: Average loss: 3.1297\tAccuracy: 0.0039\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-f28802dffa82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrease_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'increase_delta'"],"ename":"AttributeError","evalue":"'Adam' object has no attribute 'increase_delta'","output_type":"error"}]},{"cell_type":"code","source":"snapshot('/kaggle/working', run_name, {\n            'epoch': 1,\n            'validation_acc': 1, \n            'state_dict': {},\n            'validation_loss': 5,\n            'optimizer': {},\n        })","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}